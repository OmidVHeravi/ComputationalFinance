{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHLSfeKjniDHlPqSysSWMN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "WENG6CAqJwvm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def fetch_latest_data():\n",
        "    end_date = datetime.date.today().strftime('%Y-%m-%d')  # Gets the current date\n",
        "    start_date = (datetime.date.today() - datetime.timedelta(days=730)).strftime('%Y-%m-%d')  # Approx. 2 years ago\n",
        "\n",
        "    data = yf.download(\"ES=F\", start=start_date, end=end_date, interval=\"1d\")\n",
        "    data = data.dropna()\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "40raH2nFJ4lr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "def preprocess_data(data):\n",
        "    # Exclude 'Close' column from normalization\n",
        "    features = data.drop(columns=['Close'])\n",
        "\n",
        "    # Normalize features\n",
        "    scaler_features = MinMaxScaler()\n",
        "    features_normalized = scaler_features.fit_transform(features)\n",
        "\n",
        "    # Normalize 'Close' column separately\n",
        "    scaler_close = MinMaxScaler()\n",
        "    close_normalized = scaler_close.fit_transform(data[['Close']])\n",
        "\n",
        "    # Combine normalized features and close column\n",
        "    data_normalized = np.hstack([features_normalized, close_normalized])\n",
        "    data = pd.DataFrame(data_normalized, columns=data.columns)\n",
        "\n",
        "    # Outlier detection\n",
        "    z_scores = np.abs(zscore(data))\n",
        "    data = data[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "    return data, scaler_close\n"
      ],
      "metadata": {
        "id": "cE2wVJNqJ7Al"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(data):\n",
        "    # Technical Indicators\n",
        "    data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
        "    data['SMA_10'] = data['Close'].rolling(window=10).mean()\n",
        "\n",
        "    # Time Features\n",
        "    day_of_week = []\n",
        "    month = []\n",
        "    for date_str in data.index:\n",
        "        date_obj = pd.Timestamp(date_str)\n",
        "        day_of_week.append(date_obj.weekday())\n",
        "        month.append(date_obj.month)\n",
        "\n",
        "    data['Day_of_Week'] = day_of_week\n",
        "    data['Month'] = month\n",
        "\n",
        "    # Rolling Statistics\n",
        "    data['Rolling_Mean'] = data['Close'].rolling(window=5).mean()\n",
        "    data['Rolling_Std'] = data['Close'].rolling(window=5).std()\n",
        "\n",
        "    # Exponential Moving Average\n",
        "    data['EMA_5'] = data['Close'].ewm(span=5, adjust=False).mean()\n",
        "    data['EMA_10'] = data['Close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    # Drop NaN values introduced by rolling computations\n",
        "    data = data.dropna()\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "BnoQTfmBJ-Ff"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "    def __init__(self, features, prices):\n",
        "        self.features = features\n",
        "        self.prices = prices\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, gaussian_params=None):\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.gaussian_params = gaussian_params\n",
        "\n",
        "class DecisionNode(TreeNode):\n",
        "    def __init__(self, feature_idx, threshold, data_size, left, right):\n",
        "        super().__init__(feature_idx, threshold, left, right)\n",
        "        self.data_size = data_size\n",
        "\n",
        "class LeafNode(TreeNode):\n",
        "    def __init__(self, gaussian_params):\n",
        "        super().__init__(gaussian_params=gaussian_params)"
      ],
      "metadata": {
        "id": "8vocfm_CKCj6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "f1i41VhrJIIA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def bayesian_update(prior_mean, prior_variance, likelihood_mean, likelihood_variance, data_size):\n",
        "    epsilon = 1e-8  # small value to prevent division by zero\n",
        "\n",
        "    # Prevent division by zero\n",
        "    if likelihood_variance < epsilon:\n",
        "        likelihood_variance = epsilon\n",
        "\n",
        "    precision_prior = 1 / prior_variance\n",
        "    precision_likelihood = data_size / likelihood_variance\n",
        "    posterior_variance = 1 / (precision_prior + precision_likelihood)\n",
        "\n",
        "    # Calculate the posterior mean\n",
        "    posterior_mean = (prior_mean / prior_variance + likelihood_mean * data_size / likelihood_variance) * posterior_variance\n",
        "    return posterior_mean, posterior_variance\n",
        "\n",
        "def compute_likelihood(data, mean, variance):\n",
        "    epsilon = 1e-8\n",
        "    likelihoods = np.exp(-((data.prices - mean)**2) / (2 * (variance + epsilon))) / np.sqrt(2 * np.pi * (variance + epsilon))\n",
        "\n",
        "    # Replace zeros with a small value\n",
        "    likelihoods[likelihoods < epsilon] = epsilon\n",
        "    return likelihoods\n",
        "\n",
        "\n",
        "def hypothetical_split(data, feature_idx, threshold):\n",
        "    left_mask = data.features[:, feature_idx] < threshold\n",
        "    right_mask = ~left_mask\n",
        "    left_data = Data(data.features[left_mask], data.prices[left_mask])\n",
        "    right_data = Data(data.features[right_mask], data.prices[right_mask])\n",
        "    return left_data, right_data\n",
        "\n",
        "def bic_or_expected_utility(data, mean, variance):\n",
        "    n = len(data.prices)\n",
        "    log_likelihood = np.sum(np.log(compute_likelihood(data, mean, variance)))\n",
        "    return log_likelihood - 1/2 * np.log(n)\n",
        "\n",
        "def select_best_split(data, max_features=None):\n",
        "    best_score = float('-inf')\n",
        "    best_split = None\n",
        "    n, m = data.features.shape\n",
        "    if max_features is None:\n",
        "        features_to_check = range(m)\n",
        "    else:\n",
        "        features_to_check = np.random.choice(m, max_features, replace=False)\n",
        "    for feature_idx in features_to_check:\n",
        "        thresholds = np.unique(data.features[:, feature_idx])\n",
        "        for threshold in thresholds:\n",
        "            left_data, right_data = hypothetical_split(data, feature_idx, threshold)\n",
        "            if len(left_data.prices) == 0 or len(right_data.prices) == 0:\n",
        "                continue\n",
        "            prior_mean, prior_variance = 0, 1\n",
        "            left_likelihood_mean, left_likelihood_variance = np.mean(left_data.prices), np.var(left_data.prices)\n",
        "            right_likelihood_mean, right_likelihood_variance = np.mean(right_data.prices), np.var(right_data.prices)\n",
        "            left_post_mean, left_post_var = bayesian_update(prior_mean, prior_variance, left_likelihood_mean, left_likelihood_variance, len(left_data.prices))\n",
        "            right_post_mean, right_post_var = bayesian_update(prior_mean, prior_variance, right_likelihood_mean, right_likelihood_variance, len(right_data.prices))\n",
        "            score = bic_or_expected_utility(left_data, left_post_mean, left_post_var) + bic_or_expected_utility(right_data, right_post_mean, right_post_var)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_split = (feature_idx, threshold)\n",
        "    return best_split\n",
        "\n",
        "def bayesian_decision_tree(data, max_depth, max_features=None):\n",
        "    if max_depth == 0:\n",
        "        likelihood_mean, likelihood_variance = np.mean(data.prices), np.var(data.prices)\n",
        "        prior_mean, prior_variance = 0, 1\n",
        "        post_mean, post_var = bayesian_update(prior_mean, prior_variance, likelihood_mean, likelihood_variance, len(data.prices))\n",
        "        return LeafNode((post_mean, post_var))\n",
        "    best_split = select_best_split(data, max_features)\n",
        "    if best_split is None:\n",
        "        likelihood_mean, likelihood_variance = np.mean(data.prices), np.var(data.prices)\n",
        "        prior_mean, prior_variance = 0, 1\n",
        "        post_mean, post_var = bayesian_update(prior_mean, prior_variance, likelihood_mean, likelihood_variance, len(data.prices))\n",
        "        return LeafNode((post_mean, post_var))\n",
        "    feature_idx, threshold = best_split\n",
        "    left_data, right_data = hypothetical_split(data, feature_idx, threshold)\n",
        "    left_child = bayesian_decision_tree(left_data, max_depth - 1, max_features)\n",
        "    right_child = bayesian_decision_tree(right_data, max_depth - 1, max_features)\n",
        "    return DecisionNode(feature_idx, threshold, len(data.prices), left_child, right_child)\n",
        "\n",
        "def bootstrap(data, size=None):\n",
        "    if size is None:\n",
        "        size = len(data.prices)\n",
        "    indices = np.random.choice(len(data.prices), size, replace=True)\n",
        "    return Data(data.features[indices], data.prices[indices])\n",
        "\n",
        "def random_bayesian_forest(data, n_trees, max_depth, max_features=None):\n",
        "    trees = []\n",
        "    for _ in range(n_trees):\n",
        "        bootstrapped_data = bootstrap(data)\n",
        "        tree = bayesian_decision_tree(bootstrapped_data, max_depth, max_features)\n",
        "        trees.append(tree)\n",
        "    return trees\n",
        "\n",
        "def traverse_tree(node, sample):\n",
        "    if node.gaussian_params:  # LeafNode\n",
        "        mean, variance = node.gaussian_params\n",
        "        return np.random.normal(mean, np.sqrt(variance))\n",
        "    feature_idx, threshold = node.feature_idx, node.threshold\n",
        "    if sample[feature_idx] < threshold:\n",
        "        return traverse_tree(node.left, sample)\n",
        "    else:\n",
        "        return traverse_tree(node.right, sample)\n",
        "\n",
        "def predict_forest(forest, sample):\n",
        "    predictions = [traverse_tree(tree, sample) for tree in forest]\n",
        "    return np.mean(predictions)\n",
        "\n",
        "def evaluate_model(forest, test_data):\n",
        "    predictions = [predict_forest(forest, sample) for sample in test_data.features]\n",
        "    rmse = np.sqrt(mean_squared_error(test_data.prices, predictions))\n",
        "    mae = mean_absolute_error(test_data.prices, predictions)\n",
        "    return rmse, mae\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_feature_engineering(data):\n",
        "    # Previous prices (lags)\n",
        "    for i in range(1, 4):  # Add 3 lags\n",
        "        data[f'lag_{i}'] = data['Close'].shift(i)\n",
        "\n",
        "    # Rolling statistics\n",
        "    data['rolling_mean_3'] = data['Close'].rolling(window=3).mean()\n",
        "    data['rolling_std_3'] = data['Close'].rolling(window=3).std()\n",
        "\n",
        "    data.dropna(inplace=True)  # Drop rows with NaN values due to lag and rolling features\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "aP71ctQYSBGN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_window_backtest(data, window_size, model_func):\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    for end in range(window_size + 1, len(data) + 1):\n",
        "        train_data = data.iloc[end - window_size - 1:end - 1]\n",
        "        test_data = data.iloc[end - 1:end]\n",
        "\n",
        "        X_train = train_data.drop(columns=['Close']).values\n",
        "        y_train = train_data['Close'].values\n",
        "        X_test = test_data.drop(columns=['Close']).values\n",
        "        y_test = test_data['Close'].values\n",
        "\n",
        "        forest = model_func(X_train, y_train)\n",
        "\n",
        "        prediction = predict_forest(forest, X_test[0])\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        actuals.append(y_test[0])\n",
        "\n",
        "    return predictions, actuals\n",
        "\n"
      ],
      "metadata": {
        "id": "0i4J2Jq7SB1U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rbf(X_train, y_train):\n",
        "\n",
        "    train_data = Data(X_train, y_train)\n",
        "    forest = random_bayesian_forest(train_data, n_trees=10, max_depth=3)\n",
        "    return forest\n"
      ],
      "metadata": {
        "id": "tsktcIyQUAmK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_price_for_today(data, window_size, scaler_close):\n",
        "    \"\"\"\n",
        "    Predicts the price for today using the most recent data of size window_size.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The processed data with features.\n",
        "    - window_size: The size of the window (number of days) to use for training.\n",
        "    - scaler_close: The MinMaxScaler object fitted to the 'Close' column.\n",
        "\n",
        "    Returns:\n",
        "    - Predicted price for today in original scale.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the most recent data\n",
        "    latest_data = data.iloc[-window_size:]\n",
        "\n",
        "    # Train the Random Bayesian Forest\n",
        "    X_latest = latest_data.drop(columns=['Close']).values\n",
        "    y_latest = latest_data['Close'].values\n",
        "    forest = train_rbf(X_latest, y_latest)\n",
        "\n",
        "    # Predict using today's features\n",
        "    today_features = data.iloc[-1:].drop(columns=['Close']).values\n",
        "    predicted_price_for_today = predict_forest(forest, today_features[0])\n",
        "\n",
        "    # Convert to original scale\n",
        "    predicted_price_for_today_original_scale = scaler_close.inverse_transform([[predicted_price_for_today]])[0][0]\n",
        "\n",
        "    return predicted_price_for_today_original_scale\n"
      ],
      "metadata": {
        "id": "NIpJPkGIVLxH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fetch the latest data\n",
        "data = fetch_latest_data()\n",
        "\n",
        "# 2. Preprocess and feature engineering\n",
        "data, scaler_close = preprocess_data(data)\n",
        "data = enhanced_feature_engineering(data)\n",
        "\n",
        "# 3. Train-Test Split\n",
        "#X = data.drop(columns=['Close']).values\n",
        "#y = data['Close'].values\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "#train_data = Data(X_train, y_train)\n",
        "#test_data = Data(X_test, y_test)\n",
        "\n",
        "# 4. Model Training\n",
        "#forest = random_bayesian_forest(train_data, n_trees=10, max_depth=3)\n",
        "\n",
        "predictions, actuals = rolling_window_backtest(data, window_size=60, model_func=train_rbf)  # Assuming 60 days for training\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "print(f\"RMSE: {rmse}, MAE: {mae}\")\n",
        "\n",
        "predicted_today = predict_price_for_today(data, window_size=60, scaler_close=scaler_close)\n",
        "print(f\"Predicted price for today: {predicted_today}\")\n",
        "\n",
        "# 5. Model Evaluation\n",
        "#rmse, mae = evaluate_model(forest, test_data)\n",
        "#print(f\"RMSE: {rmse}, MAE: {mae}\")\n",
        "\n",
        "# 6. Prediction for today\n",
        "#X_latest = data.iloc[-1:].drop(columns=['Close']).values\n",
        "#predicted_price = predict_forest(forest, X_latest[0])\n",
        "#predicted_price_original_scale = scaler_close.inverse_transform([[predicted_price]])[0][0]\n",
        "#print(f\"Predicted price for today in original scale: {predicted_price_original_scale}\")\n",
        "\n",
        "#predicted_price_normalized = 0.7476125257069268  # This is just an example value\n",
        "#predicted_price_original_scale = scaler_close.inverse_transform([[predicted_price_normalized]])[0][0]\n",
        "#print(f\"Predicted price for today in original scale: {predicted_price_original_scale}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM3uBtfWKOq9",
        "outputId": "7f1d5456-1a21-4ff1-89eb-117b84bd6fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz\n"
      ],
      "metadata": {
        "id": "UyV70vZ1OoB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from graphviz import Digraph"
      ],
      "metadata": {
        "id": "8Fiyit2lQiFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_tree(node, feature_names=None, parent_name=None, graph=None, edge_label=None):\n",
        "    if graph is None:\n",
        "        graph = Digraph('BayesianTree', node_attr={'style': 'filled'})\n",
        "\n",
        "    if isinstance(node, LeafNode):\n",
        "        mean, variance = node.gaussian_params\n",
        "        graph.node(name=str(id(node)),\n",
        "                   label=f\"Mean: {mean:.2f}\\nVar: {variance:.2f}\",\n",
        "                   color='lightyellow')\n",
        "\n",
        "    elif isinstance(node, DecisionNode):\n",
        "        description = feature_names[node.feature_idx] if feature_names is not None else node.feature\n",
        "        graph.node(name=str(id(node)),\n",
        "                   label=f\"{description} <= {node.threshold:.2f}\\nSamples: {node.data_size}\",\n",
        "                   color='lightblue')\n",
        "\n",
        "        # Left subtree (True branch)\n",
        "        visualize_tree(node.left, feature_names, str(id(node)), graph, 'True')\n",
        "\n",
        "        # Right subtree (False branch)\n",
        "        visualize_tree(node.right, feature_names, str(id(node)), graph, 'False')\n",
        "\n",
        "        if parent_name:\n",
        "            graph.edge(parent_name, str(id(node)), label=edge_label)\n",
        "    else:\n",
        "        print(f\"Unexpected node type: {type(node)}\")\n",
        "        print(node.__dict__)  # print attributes of the node\n",
        "\n",
        "    return graph\n"
      ],
      "metadata": {
        "id": "Hkq9Jj2pPZgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "#tree_graph = visualize_tree(forest[0], feature_names=data.columns[:-1])\n",
        "#tree_graph.view(\"Bayesian_Tree\")\n",
        "\n",
        "tree_graph = visualize_tree(forest[0], feature_names=data.columns[:-1])\n",
        "tree_graph"
      ],
      "metadata": {
        "id": "zXmXsLyMRy9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}